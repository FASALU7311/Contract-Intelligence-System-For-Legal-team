# import os
# os.environ["TRANSFORMERS_NO_TF"] = "1"

# import streamlit as st
# import torch
# import pandas as pd
# import fitz  # PyMuPDF
# import re
# from transformers import (
#     BertTokenizer, BertForSequenceClassification
# )
# from streamlit_lottie import st_lottie
# import requests
# from streamlit_option_menu import option_menu
# from fpdf import FPDF
# import hashlib
# import smtplib
# from email.mime.text import MIMEText
# from email.mime.multipart import MIMEMultipart
# import random
# from io import BytesIO
# import docx
# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
# import uuid
# from datetime import datetime, timedelta
# from streamlit_cookies_manager import EncryptedCookieManager
# import time 
# import json
# import os

# st.set_page_config(page_title="Contract Intelligence System", layout="wide", page_icon="üìÑ")
# # ---------- Persistence helpers (disk + cookie) ----------
# DATA_DIR = "persist"
# os.makedirs(DATA_DIR, exist_ok=True)

# def _analysis_path(key: str) -> str:
#     return os.path.join(DATA_DIR, f"{key}.json")

# def persist_analysis_to_disk(df: pd.DataFrame) -> str:
#     """Save analysis to disk and return a short key."""
#     # one key per user; use uuid to avoid clashes
#     user = st.session_state.get("user", "anon").replace("@", "_").replace(".", "_")
#     key = f"{user}_{int(time.time())}_{uuid.uuid4().hex[:8]}"
#     df.to_json(_analysis_path(key), orient="records")
#     # remember the key in both places
#     cookies["analysis_key"] = key
#     cookies.save()
#     st.session_state["analysis_key"] = key
#     return key

# def restore_analysis_from_cookie() -> pd.DataFrame | None:
#     """Load analysis from disk using the key stored in cookie/session."""
#     key = cookies.get("analysis_key") or st.session_state.get("analysis_key")
#     if not key:
#         return None
#     path = _analysis_path(key)
#     if os.path.exists(path):
#         try:
#             return pd.read_json(path)
#         except Exception:
#             return None
#     return None

# def clear_persisted_analysis():
#     """Delete analysis file and clear cookie/session."""
#     key = cookies.get("analysis_key") or st.session_state.get("analysis_key")
#     if key:
#         path = _analysis_path(key)
#         if os.path.exists(path):
#             try:
#                 os.remove(path)
#             except Exception:
#                 pass
#     cookies["analysis_key"] = ""
#     cookies.save()
#     st.session_state.pop("analysis_key", None)
#     st.session_state.pop("clause_data", None)
# # ---------------------------------------------------------

# # --- Helper Functions ---
# def load_lottieurl(url):
#     r = requests.get(url)
#     if r.status_code != 200:
#         return None
#     return r.json()

# # --- Authentication UI ---
# def register_page():
#     st.subheader("Register")
#     username = st.text_input("Username", key="reg_username")
#     email = st.text_input("Gmail Address", key="reg_email")
#     password = st.text_input("Password", type="password", key="reg_pass")
#     if st.button("Send OTP"):
#         if not email.endswith("@gmail.com"):
#             st.error("Please use a valid Gmail address.")
#         elif user_exists(email):
#             st.error("User already exists.")
#         elif username_exists(username):
#             st.error("Username already taken.")
#         else:
#             otp = generate_otp()
#             # Always send OTP to the email entered in the form
#             if send_otp_email(email, otp):
#                 df = load_users()
#                 new_row = pd.DataFrame([{"username": username, "email": email, "password_hash": hash_password(password), "otp": otp, "is_verified": False}])
#                 df = pd.concat([df, new_row], ignore_index=True)
#                 save_users(df)
#                 st.session_state["pending_email"] = email
#                 st.success(f"OTP sent to {email}. Please verify.")
#             else:
#                 st.error("Failed to send OTP. Check email config.")

#     # Always show OTP input if Send OTP was clicked (pending_email in session)
#     if "pending_email" in st.session_state:
#         st.markdown("**Enter the OTP sent to your Gmail to complete registration:**")
#         otp_input = st.text_input("Enter OTP for Registration", key="reg_otp")
#         if st.button("Verify OTP"):
#             user = get_user(st.session_state["pending_email"])
#             if user is not None and str(user["otp"]) == otp_input:
#                 verify_user(user["email"])
#                 st.success("Registration complete! You can now login.")
#                 del st.session_state["pending_email"]
#             else:
#                 st.error("Invalid OTP.")
# # --- Cookie Manager ---
# cookies = EncryptedCookieManager(
#     prefix="contract_intel_",  
#     password="super_secret_key_here"  # change to secure random string
# )
# if not cookies.ready():
#     st.stop()
    
# # Default to upload page if no cookie exists
# current_page = cookies.get("current_page", "Upload")

# # --- Session State Restore ---
# if "user" not in st.session_state:
#     # Try to restore from cookies
#     if cookies.get("user") and cookies.get("username"):
#         st.session_state["user"] = cookies.get("user")
#         st.session_state["username"] = cookies.get("username")
#         st.session_state["authenticated"] = True
#     else:
#         st.session_state["authenticated"] = False
# # --- LOGIN PAGE ---
# def login_page():
#     st.subheader("Login")
#     username = st.text_input("Username", key="login_username")
#     password = st.text_input("Password", type="password", key="login_pass")

#     col1, col2 = st.columns([2, 1])
#     login_clicked = col1.button("Login")
#     forgot_clicked = col2.button("Forgot Password")

#     if login_clicked:
#         user = get_user_by_username(username)
#         if user is None:
#             st.error("User not found.")
#         elif user["password_hash"] != hash_password(password):
#             st.error("Incorrect password.")
#         elif not user["is_verified"]:
#             st.error("User not verified. Please register and verify OTP.")
#         else:
#             # Save to session
#             st.session_state["authenticated"] = True
#             st.session_state["user"] = user["email"]
#             st.session_state["username"] = user["username"]

#             # Save to cookies (persist login)
#             cookies["user"] = user["email"]
#             cookies["username"] = user["username"]
#             cookies.save()

#             st.success("Login successful!")
#             time.sleep(0.5)
#             st.rerun()

#     if forgot_clicked:
#         st.session_state["show_forgot_password"] = True
#         st.session_state["authenticated"] = False
#         st.rerun()

# # --- LOGOUT ---
# def logout():
#     for key in ["authenticated", "user", "username", "show_forgot_password"]:
#         if key in st.session_state:
#             del st.session_state[key]

#     # Clear cookies
#     cookies["user"] = ""
#     cookies["username"] = ""
#     cookies.save()

#     st.success("Logged out successfully!")
#     time.sleep(0.5)
#     st.rerun()

# def forgot_password_page():
#     st.subheader("Forgot Password")
#     # Step 1: Username and Email
#     if "fp_stage" not in st.session_state:
#         st.session_state["fp_stage"] = "input_user_email"

#     if st.session_state["fp_stage"] == "input_user_email":
#         username = st.text_input("Username", key="fp_username")
#         email = st.text_input("Registered Gmail", key="fp_email")
#         if st.button("Send OTP for Reset"):
#             user = get_user_by_username(username)
#             if user is None:
#                 st.error("User not found.")
#             elif user["email"] != email:
#                 st.error("Email does not match the username.")
#             else:
#                 otp = generate_otp()
#                 update_user_otp(email, otp)
#                 if send_otp_email(email, otp):
#                     st.session_state["fp_pending_email"] = email
#                     st.session_state["fp_stage"] = "otp_and_password"
#                     st.success("OTP sent to your Gmail.")
#                 else:
#                     st.error("Failed to send OTP.")

#     # Step 2: OTP and New Password
#     if st.session_state.get("fp_stage") == "otp_and_password":
#         st.markdown("**Enter the OTP sent to your Gmail to reset your password:**")
#         otp_input = st.text_input("Enter OTP for Password Reset", key="fp_otp")
#         new_pass = st.text_input("New Password", type="password", key="fp_newpass")
#         confirm_pass = st.text_input("Confirm New Password", type="password", key="fp_confirmpass")
#         if st.button("Reset Password"):
#             user = get_user(st.session_state["fp_pending_email"])
#             if user is not None and str(user["otp"]) == otp_input:
#                 if new_pass != confirm_pass:
#                     st.error("Passwords do not match.")
#                 elif len(new_pass) < 6:
#                     st.error("Password must be at least 6 characters.")
#                 else:
#                     update_user_password(user["email"], new_pass)
#                     st.success("Password reset successful! You can now login.")
#                     del st.session_state["fp_pending_email"]
#                     st.session_state["fp_stage"] = "input_user_email"
#             else:
#                 st.error("Invalid OTP.")

# # --- User Data Functions ---
# USERS_CSV = "users.csv"

# def load_users():
#     cols = ["username", "email", "password_hash", "otp", "is_verified"]
#     if not os.path.exists(USERS_CSV):
#         return pd.DataFrame(columns=cols)
#     df = pd.read_csv(USERS_CSV)
#     # Migrate legacy or incomplete files
#     for col in cols:
#         if col not in df.columns:
#             df[col] = "" if col != "is_verified" else False
#     # Ensure correct column order
#     df = df[cols]
#     return df

# def save_users(df):
#     df.to_csv(USERS_CSV, index=False)

# def hash_password(password):
#     return hashlib.sha256(password.encode()).hexdigest()

# def generate_otp():
#     return str(random.randint(100000, 999999))

# def send_otp_email(receiver_email, otp):
#     # Configure your Gmail credentials here
#     sender_email = "learnsaxon@gmail.com"  # <-- CHANGE THIS
#     sender_password = "ihiumsydgpaceykb"
#     subject = "Your OTP Code"
#     body = f"Your OTP code is: {otp}"
#     msg = MIMEMultipart()
#     msg["From"] = sender_email
#     msg["To"] = receiver_email
#     msg["Subject"] = subject
#     msg.attach(MIMEText(body, "plain"))
#     try:
#         server = smtplib.SMTP("smtp.gmail.com", 587)
#         server.starttls()
#         server.login(sender_email, sender_password)
#         server.sendmail(sender_email, receiver_email, msg.as_string())
#         server.quit()
#         return True
#     except Exception as e:
#         print("Email send error:", e)
#         return False

# def user_exists(email):
#     df = load_users()
#     return email in df["email"].values

# def username_exists(username):
#     df = load_users()
#     return username in df["username"].values

# def get_user(email):
#     df = load_users()
#     user = df[df["email"] == email]
#     return user.iloc[0] if not user.empty else None

# def get_user_by_username(username):
#     df = load_users()
#     user = df[df["username"] == username]
#     return user.iloc[0] if not user.empty else None

# def update_user_otp(email, otp):
#     df = load_users()
#     df.loc[df["email"] == email, "otp"] = otp
#     save_users(df)

# def update_user_password(email, new_password):
#     df = load_users()
#     df.loc[df["email"] == email, "password_hash"] = hash_password(new_password)
#     save_users(df)

# def verify_user(email):
#     df = load_users()
#     df.loc[df["email"] == email, "is_verified"] = True
#     save_users(df)

# # --- Page Setup ---
# st.markdown("""
#     <style>
#     .main { background-color: #f8f9fa; }
#     h1, h2, h3 { color: #2e86de; }
#     .sidebar .sidebar-content { background-color: #1e272e; color: white; }
#     .css-1d391kg { padding: 2rem; border-radius: 10px; }
#     </style>
# """, unsafe_allow_html=True)
# lottie_contract = load_lottieurl("https://assets10.lottiefiles.com/packages/lf20_t9gkkhz4.json")

# # --- Auth Navigation ---
# if not st.session_state.get("authenticated", False):
#     if st.session_state.get("show_forgot_password", False):
#         forgot_password_page()
#         if st.button("Back to Login"):
#             st.session_state["show_forgot_password"] = False
#             st.rerun()
#     else:
#         auth_tab = st.sidebar.radio("Authentication", ["Login", "Register", "Forgot Password"])
#         if auth_tab == "Login":
#             login_page()
#         elif auth_tab == "Register":
#             register_page()
#         elif auth_tab == "Forgot Password":
#             st.session_state["show_forgot_password"] = True
#             st.rerun()
#     st.stop()
# else:
#     st.sidebar.markdown(f"**Logged in as:** {st.session_state.get('username', '')}")
#     if st.sidebar.button("‚Ü© Logout"):
#         logout()

# # --- Navigation Menu ---
# # Ensure session state has a 'current_page' key
# if "current_page" not in st.session_state:
#     st.session_state["current_page"] = cookies.get("current_page", "Upload")

# with st.sidebar:
#     selected = option_menu(
#         "Navigation",
#         ["Upload", "Analysis", "Insights", "Download"],
#         icons=['cloud-upload', 'bar-chart', 'lightbulb', 'download'],
#         menu_icon="cast",
#         default_index=["Upload", "Analysis", "Insights", "Download"].index(st.session_state["current_page"]),
#         key="main_navigation" # Use a key for a stateful widget
#     )

# # Logic to handle page changes
# if selected != st.session_state["current_page"]:
#     st.session_state["current_page"] = selected
#     cookies["current_page"] = selected
#     cookies.save()
#     st.rerun() # Rerun the app to switch the page immediately
# st.title("üóê ‚ÑÇùï†ùïüùï•ùï£ùïíùïîùï• ùïÄùïüùï•ùïñùïùùïùùïöùïòùïñùïüùïîùïñ ùïäùï™ùï§ùï•ùïñùïû ùïóùï†ùï£ ùïÉùïñùïòùïíùïù ùïãùïñùïíùïûùï§")

# # --- Load Legal-BERT Model ---
# @st.cache_resource
# def load_bert_model():
#     model_path = "./legalbert_model"
#     if not os.path.exists(model_path):
#         st.error(f"Model directory '{model_path}' not found. Please ensure the model files are present.")
#         return None, None
#     try:
#         bert_model = BertForSequenceClassification.from_pretrained(model_path)
#         bert_tokenizer = BertTokenizer.from_pretrained(model_path)
#         bert_model.eval()
#         return bert_model, bert_tokenizer
#     except Exception as e:
#         st.error(f"Failed to load Legal-BERT model: {e}")
#         return None, None

# bert_model, bert_tokenizer = load_bert_model()
# if not bert_model or not bert_tokenizer:
#     st.stop()

# # --- Load Legal Summarizer ---
# @st.cache_resource
# def load_pegasus_model():
#     pegasus_model_name = "google/pegasus-xsum"
#     try:
#         pegasus_tokenizer = AutoTokenizer.from_pretrained(pegasus_model_name)
#         pegasus_model = AutoModelForSeq2SeqLM.from_pretrained(pegasus_model_name)
#         pegasus_model.eval()
#         return pegasus_model, pegasus_tokenizer
#     except Exception as e:
#         st.error(f"Failed to load PEGASUS model: {e}")
#         return None, None

# pegasus_model, pegasus_tokenizer = load_pegasus_model()
# if not pegasus_model or not pegasus_tokenizer:
#     st.stop()

# @st.cache_data
# def load_label_mapping():
#     if not os.path.exists("label_mapping.csv"):
#         st.error("`label_mapping.csv` not found. Please provide the mapping file.")
#         return None
#     try:
#         label_map_df = pd.read_csv("label_mapping.csv")
#         label_map_df["clause_type"] = label_map_df["clause_type"].str.strip().str.replace('"', '')
#         id2label = dict(label_map_df.values)
#         return id2label
#     except Exception as e:
#         st.error(f"Error loading label mapping: {e}")
#         return None

# id2label = load_label_mapping()
# if not id2label:
#     st.stop()

# # --- Clause Insight Mapping ---
# clause_insights = {
#     "Confidentiality": "Ensures sensitive information is protected.",
#     "Termination for Convenience": "Allows parties to end the contract without cause.",
#     "Dispute Resolution": "Specifies how disputes will be resolved.",
#     "Cap on Liability": "Limits the amount a party has to pay if things go wrong.",
#     "IP Ownership": "Clarifies who owns the intellectual property.",
#     "Anti-Assignment": "Restricts transfer of contractual rights.",
#     "Governing Law": "Specifies the legal jurisdiction governing the contract.",
#     "Post-Termination": "Explains obligations even after the contract ends.",
#     "General legal clause": "Standard legal clause without specific risk."
# }

# # --- Utils ---
# @st.cache_data
# def extract_deadlines_and_risks(text):
#     text_lower = text.lower()

#     # ‚úÖ Deadline extraction patterns
#     deadline_patterns = [
#         r"(?:within|at least|not less than|no later than)?\s*\(?\d{1,3}\)?\s*(?:days?|weeks?|months?|years?)'?\s*(?:prior to|before|after|preceding|from)?\s*(?:the\s+)?(?:effective\s+date|termination|claim|execution|delivery|notice|event)?",
#         r"(?:on|by|before)?\s*(?:jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sep(?:t(?:ember)?)?|oct(?:ober)?|nov(?:ember)?|dec(?:ember)?)\s+\d{1,2},?\s+\d{4}",
#         r"(?:\d{1,2}/\d{1,2}/\d{2,4})"
#     ]

#     deadlines = []
#     for pattern in deadline_patterns:
#         found = re.findall(pattern, text, flags=re.IGNORECASE)
#         for d in found:
#             cleaned = d.strip().replace('\n', ' ')
#             if len(cleaned) > 5 and not cleaned.lower().startswith(('or', 'and')):
#                 deadlines.append(cleaned)

#     deadline_result = ", ".join(sorted(set(deadlines))) if deadlines else "-"

#     # ‚úÖ Risk keywords
#     risk_keywords = {
#         "termination": "termination",
#         "penalty": "penalty",
#         "liability": "liability",
#         "breach": "breach",
#         "damages": "damages",
#         "dispute": "dispute",
#         "loss": "loss",
#         "indemnity": "indemnity",
#         "transfer": "transfer risk",
#         "ownership": "IP risk"
#     }
#     risks = {label for word, label in risk_keywords.items() if word in text_lower}
#     risk_result = ", ".join(sorted(risks)) if risks else "-"

#     return deadline_result, risk_result

# def generate_summaries_batch(clauses, batch_size=8):
#     summaries = []
#     for i in range(0, len(clauses), batch_size):
#         batch_clauses = clauses[i:i+batch_size]
#         filtered_batch = [clause if len(clause) > 100 else "[SHORT]" for clause in batch_clauses]

#         inputs = pegasus_tokenizer(filtered_batch, truncation=True, padding="longest", return_tensors="pt")
#         with torch.no_grad():
#             summary_ids = pegasus_model.generate(
#                 **inputs, max_length=60, min_length=20, num_beams=4, early_stopping=True
#             )
#         decoded = pegasus_tokenizer.batch_decode(summary_ids, skip_special_tokens=True)

#         for original, summary in zip(batch_clauses, decoded):
#             if len(original) <= 100:
#                 summaries.append(original.strip())
#             else:
#                 summaries.append(summary.strip())
#     return summaries


# def generate_insight(clause_text, label=None):
#     text = clause_text.lower()

#     # Use clause type first if available
#     if label:
#         if label == "Confidentiality":
#             return "Ensures sensitive or proprietary information is not disclosed."
#         elif label == "Termination for Convenience":
#             return "Allows either party to terminate the agreement without specific cause."
#         elif label == "Dispute Resolution":
#             return "Describes how legal conflicts will be resolved, such as arbitration or court."
#         elif label == "Cap on Liability":
#             return "Limits the amount one party must pay for damages or losses."
#         elif label == "IP Ownership":
#             return "Specifies who holds the rights to intellectual property developed or used."
#         elif label == "Anti-Assignment":
#             return "Prevents parties from transferring their rights or obligations to others."
#         elif label == "Governing Law":
#             return "Declares which country's/state's laws apply to the contract."
#         elif label == "Post-Termination":
#             return "Describes obligations that continue even after the contract ends."
#         elif label == "General legal clause":
#             return "Standard clause included for legal completeness."

#     # Fallback to keyword check
#     if "terminate" in text:
#         return "Clause allows termination of agreement."
#     elif "jurisdiction" in text or "governed by" in text:
#         return "Specifies the legal jurisdiction."
#     elif "liability" in text:
#         return "Limits financial liability."
#     elif "confidential" in text:
#         return "Protects sensitive information."
#     elif "intellectual property" in text or "ownership" in text:
#         return "Defines intellectual property rights."
#     else:
#         return "General legal clause."

# def predict_clauses_batch(clauses):
#     tokens = bert_tokenizer(clauses, return_tensors="pt", padding=True, truncation=True, max_length=512)
#     with torch.no_grad():
#         outputs = bert_model(**tokens)
#         probs = torch.nn.functional.softmax(outputs.logits, dim=1)
#         pred_ids = torch.argmax(probs, dim=1)
#         confidences = probs.max(dim=1).values
#     labels = [id2label[i.item()] for i in pred_ids]
#     return labels, confidences.tolist()


# def split_into_clauses(text):
#     text = re.sub(r'\n+', '\n', text)
#     text = re.sub(r'\xa0', ' ', text)
#     pattern = r'(?:^|\n)(?=[0-9]+\.\s|[\u2022\-]\s|[A-Z][a-z])'
#     clauses = re.split(pattern, text)
#     return [c.strip() for c in clauses if len(c.strip()) > 40]


# def extract_text(file):
#     if file.type == "application/pdf":
#         return extract_text_from_pdf(file)
#     elif file.type == "text/plain":
#         return file.read().decode("utf-8")
#     elif file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
#         doc = docx.Document(file)
#         return "\n".join([p.text for p in doc.paragraphs])
#     elif file.type == "text/csv":
#         df = pd.read_csv(file)
#         return "\n".join(df.iloc[:,0].astype(str))
#     elif file.type == "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet":
#         df = pd.read_excel(file)
#         return "\n".join(df.iloc[:,0].astype(str))
#     return ""


# def extract_text_from_pdf(uploaded_pdf):
#     doc = fitz.open(stream=uploaded_pdf.read(), filetype="pdf")
#     return "\n".join(page.get_text() for page in doc)


# def generate_file_download(dataframe, file_format):
#     # Ensure all columns are strings for clean export
#     df_str = dataframe.astype(str)
    
#     if file_format == "PDF":
#         pdf = FPDF()
#         pdf.set_auto_page_break(auto=True, margin=15)
#         pdf.add_page()
#         pdf.set_font("Arial", size=10)
        
#         # Change the encoding to 'utf-8' for the output
#         for _, row in df_str.iterrows():
#             text = f"Clause Type: {row['Clause Type']}\nConfidence: {row['Confidence (%)']}%\nDeadline: {row['Deadline']}\nRisks: {row['Risks']}\nInsight: {row['Insight']}\nSummary: {row['Summary']}\n\n{row['Clause']}\n"
#             pdf.multi_cell(0, 10, text.encode('utf-8').decode('latin-1'), border=1) # Encode to utf-8, then decode to latin-1 for fpdf
#             pdf.ln()
            
#         # The key change is here: use a more robust encoding for the final output
#         pdf_output = pdf.output(dest='S').encode('utf-8', 'ignore') 
#         return pdf_output, "application/pdf", "predicted_clauses.pdf"
    
#     elif file_format == "TXT":
#         text_data = df_str.to_string(index=False)
#         return text_data.encode('utf-8'), "text/plain", "predicted_clauses.txt"
    
#     elif file_format == "CSV":
#         return df_str.to_csv(index=False).encode('utf-8'), "text/csv", "predicted_clauses.csv"
    
#     elif file_format == "DOCX":
#         doc = docx.Document()
#         for _, row in df_str.iterrows():
#             doc.add_paragraph(f"Clause Type: {row['Clause Type']}")
#             doc.add_paragraph(f"Confidence: {row['Confidence (%)']}%")
#             doc.add_paragraph(f"Deadline: {row['Deadline']}")
#             doc.add_paragraph(f"Risks: {row['Risks']}")
#             doc.add_paragraph(f"Insight: {row['Insight']}")
#             doc.add_paragraph(f"Summary: {row['Summary']}")
#             doc.add_paragraph(row['Clause'])
#             doc.add_paragraph("---------------------------")
#         buffer = BytesIO()
#         doc.save(buffer)
#         return buffer.getvalue(), "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "predicted_clauses.docx"
    
#     elif file_format == "XLSX":
#         buffer = BytesIO()
#         df_str.to_excel(buffer, index=False)
#         return buffer.getvalue(), "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", "predicted_clauses.xlsx"


# if selected == "Upload":
#     st_lottie(lottie_contract, height=250, key="contract")
#     uploaded_file = st.file_uploader("üìéUpload Contract (PDF, DOCX, XLSX, TXT, CSV)", type=["pdf", "txt", "docx", "csv", "xlsx"])
#     if uploaded_file:
#         raw_text = extract_text(uploaded_file)
#         st.text_area("Extracted Text", raw_text[:3000], height=250)
#         clauses = split_into_clauses(raw_text)
#         st.success(f"‚úì Found {len(clauses)} potential clauses.")  ########################################################################
#         for i, c in enumerate(clauses[:5]):
#             st.write(f"Clause {i+1}: {c[:100]}...")
#         st.session_state.clauses = clauses

# elif selected == "Analysis":
#     st.subheader("‚åï Run Clause Analysis")

#     # Check if a file has been uploaded and clauses extracted
#     if "clauses" in st.session_state and st.session_state.clauses:
#         # The st.button triggers a rerun, but we need to ensure the state is clean first
#         if st.button("‚åï Analyze Clauses"):
#             with st.spinner("Analyzing clauses using Legal-BERT and PEGASUS-XSUM..."):
#                 labels, confidences = predict_clauses_batch(st.session_state.clauses)
#                 summaries = generate_summaries_batch(st.session_state.clauses)
#                 data = []

#                 for clause, label, conf, summary in zip(st.session_state.clauses, labels, confidences, summaries):
#                     deadline, risks = extract_deadlines_and_risks(clause)
#                     insight = clause_insights.get(label, "General legal clause.")
#                     data.append({
#                         "Clause": clause,
#                         "Clause Type": label,
#                         "Confidence (%)": round(conf * 100, 2),
#                         "Deadline": deadline,
#                         "Risks": risks,
#                         "Insight": insight,
#                         "Summary": summary
#                     })
                
#                 result_df = pd.DataFrame(data)
                
#                 # Update the session state with the new analysis results
#                 st.session_state.clause_data = result_df
                
#                 # Persist the analyzed data to disk for a more robust session
#                 persist_analysis_to_disk(result_df)
                
#                 st.success("‚úì Analysis completed.")#####################################################################################
                
#                 # Optional: Rerun the app to immediately show the "Insights" or "Download" tab
#                 # st.rerun()

#     else:
#         st.warning("‚ö†Ô∏é No document uploaded. Go to 'Upload' to begin.") ################################################################



# elif selected == "Insights":
#     if "clause_data" not in st.session_state:
#         restored = restore_analysis_from_cookie()
#         if restored is not None:
#             st.session_state.clause_data = restored

#     if "clause_data" in st.session_state:
#         result_df = st.session_state.clause_data
#         st.markdown("###ìäç Overall Insights")       #######################################################################################
#         col1, col2, col3, col4 = st.columns(4)
#         with col1:
#             st.metric("Total Clauses", len(result_df))
#         with col2:
#             st.metric("Unique Clause Types", len(result_df["Clause Type"].unique()))
#         with col3:
#             st.metric("High-Risk Clauses", (result_df["Risks"] != "-").sum())
#         with col4:
#         # Count clauses where the 'Deadline' column is not "-"
#            st.metric("Clauses with Deadlines", (result_df["Deadline"] != "-").sum())

#         st.markdown("---")
#         st.markdown("### ìäç Full Clause Insights")   ######################################################################################
#         st.dataframe(result_df, use_container_width=True)

#         st.markdown("### ‚ö†Ô∏é High Risk Clauses")
#         st.dataframe(result_df[result_df["Risks"] != "-"][["Clause", "Risks"]], use_container_width=True)

#         st.markdown("### ‚è± Clauses with Deadlines")
#         st.dataframe(result_df[result_df["Deadline"] != "-"][["Clause", "Deadline"]], use_container_width=True)

#         st.markdown("### üìù Clause Summaries")
#         st.dataframe(result_df[["Clause", "Summary"]], use_container_width=True)

#         if result_df["Summary"].eq(result_df["Clause"]).all():
#             st.warning("‚ö†Ô∏é All summaries are identical to clauses. Try uploading longer clauses or more complex contracts.")
#     else:
#         st.warning("‚ö†Ô∏é No analyzed data found. Go to 'Analysis' tab.")

# elif selected == "Download":
#     if "clause_data" in st.session_state:
#         result_df = st.session_state.clause_data
#         file_format = st.selectbox("Select Format", ["PDF", "DOCX", "XLSX", "TXT", "CSV"])
#         file_data, mime, filename = generate_file_download(result_df, file_format)
#         st.download_button(f"‚¨áÔ∏è Download Clause Report ({file_format})", file_data, filename, mime)
#     else:
#         st.warning("‚ö†Ô∏é Nothing to download. Run analysis first.")  ##################################################################

# # --- Sidebar: Clause Repository ---
# st.sidebar.title("üìö Clause Repository")

# # üîπ Add Clear Saved Analysis button at the top
# if st.sidebar.button("üóëÔ∏è Clear Saved Analysis"):
#     clear_persisted_analysis()
#     st.session_state.pop("clause_data", None)
#     st.success("Analysis data cleared.")

# if "clause_data" in st.session_state:
#     clause_df = st.session_state.clause_data
#     clause_types = sorted(clause_df["Clause Type"].unique())

#     # --- Filters ---
#     selected_type = st.sidebar.selectbox("üîé Filter by Clause Type", ["All"] + clause_types)
#     search_query = st.sidebar.text_input("üîç Search Clause or Summary")

#     # --- Apply Filters ---
#     filtered_df = clause_df.copy()
#     if selected_type != "All":
#         filtered_df = filtered_df[filtered_df["Clause Type"] == selected_type]
#     if search_query:
#         filtered_df = filtered_df[
#             filtered_df["Clause"].str.contains(search_query, case=False, na=False) |
#             filtered_df["Summary"].str.contains(search_query, case=False, na=False)
#         ]

#     st.sidebar.markdown(f"**üóê Total Filtered Clauses: {len(filtered_df)}**")

#     # --- Display Clauses with Full Info ---
#     for i, row in filtered_df.iterrows():
#         with st.sidebar.expander(f"üóê Clause {i+1}: {row['Clause Type']}"):
#             st.markdown(f"**Clause:** {row['Clause']}")
#             st.markdown(f"**Summary:** {row['Summary']}")
#             st.markdown(f"**Insight:** {row['Insight']}")
#             st.markdown(f"**Risks:** {row['Risks'] if row['Risks'] != '-' else 'No specific risk detected'}**")

#             # ‚úÖ More meaningful deadline display
#             if row['Deadline'] and row['Deadline'] != "-":
#                 st.markdown(f"**Deadline:** {row['Deadline']} üìÖ")
#             else:
#                 st.markdown("**Deadline:** ‚ùå Not specified")

#             st.markdown(f"**Confidence:** {row['Confidence (%)']}%")
# else:
#     st.sidebar.info("üóê Upload and analyze a document to view repository.")

import os
os.environ["TRANSFORMERS_NO_TF"] = "1"

import streamlit as st
import torch
import pandas as pd
import fitz  # PyMuPDF
import re
from transformers import (
    BertTokenizer, BertForSequenceClassification
)
from streamlit_lottie import st_lottie
import requests
from streamlit_option_menu import option_menu
from fpdf import FPDF
import hashlib
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import random
from io import BytesIO
import docx
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import uuid
from datetime import datetime, timedelta
from streamlit_cookies_manager import EncryptedCookieManager
import time
import json
import os

st.set_page_config(page_title="Contract Intelligence System", layout="wide", page_icon="üóê")

# Get the directory of the script
script_dir = os.path.dirname(__file__)
css_path = os.path.join(script_dir, "style.css")

# Load CSS file
with open(css_path, "r", encoding="utf-8") as f:
    css = f.read()

# Inject CSS
st.markdown(f"<style>{css}</style>", unsafe_allow_html=True)

# ---------- Persistence helpers (disk + cookie) ----------
DATA_DIR = "persist"
os.makedirs(DATA_DIR, exist_ok=True)

def _analysis_path(key: str) -> str:
    return os.path.join(DATA_DIR, f"{key}.json")

def persist_analysis_to_disk(df: pd.DataFrame) -> str:
    user = st.session_state.get("user", "anon").replace("@", "_").replace(".", "_")
    key = f"{user}_{int(time.time())}_{uuid.uuid4().hex[:8]}"
    df.to_json(_analysis_path(key), orient="records")
    cookies["analysis_key"] = key
    cookies.save()
    st.session_state["analysis_key"] = key
    return key

def restore_analysis_from_cookie() -> pd.DataFrame | None:
    key = cookies.get("analysis_key") or st.session_state.get("analysis_key")
    if not key:
        return None
    path = _analysis_path(key)
    if os.path.exists(path):
        try:
            return pd.read_json(path)
        except Exception:
            return None
    return None

def clear_persisted_analysis():
    key = cookies.get("analysis_key") or st.session_state.get("analysis_key")
    if key:
        path = _analysis_path(key)
        if os.path.exists(path):
            try:
                os.remove(path)
            except Exception:
                pass
    cookies["analysis_key"] = ""
    cookies.save()
    st.session_state.pop("analysis_key", None)
    st.session_state.pop("clause_data", None)

# --- Helper Functions ---
def load_lottieurl(url):
    r = requests.get(url)
    if r.status_code != 200:
        return None
    return r.json()

# --- Authentication UI ---
def register_page():
    st.subheader("Register")
    with st.form(key="register_form", clear_on_submit=True):
        username = st.text_input("Username", key="reg_username")
        email = st.text_input("Gmail Address", key="reg_email")
        password = st.text_input("Password", type="password", key="reg_pass")
        submit = st.form_submit_button("Send OTP")
        if submit:
            if not email.endswith("@gmail.com"):
                st.error("Please use a valid Gmail address.")
            elif user_exists(email):
                st.error("User already exists.")
            elif username_exists(username):
                st.error("Username already taken.")
            else:
                otp = generate_otp()
                if send_otp_email(email, otp):
                    df = load_users()
                    new_row = pd.DataFrame([{"username": username, "email": email, "password_hash": hash_password(password), "otp": otp, "is_verified": False}])
                    df = pd.concat([df, new_row], ignore_index=True)
                    save_users(df)
                    st.session_state["pending_email"] = email
                    st.success(f"OTP sent to {email}. Please verify.")
                else:
                    st.error("Failed to send OTP. Check email config.")

    if "pending_email" in st.session_state:
        with st.form(key="verify_form", clear_on_submit=True):
            otp_input = st.text_input("Enter OTP for Registration", key="reg_otp")
            if st.form_submit_button("Verify OTP"):
                user = get_user(st.session_state["pending_email"])
                if user is not None and str(user["otp"]) == otp_input:
                    verify_user(user["email"])
                    st.success("Registration complete! You can now login.")
                    del st.session_state["pending_email"]
                else:
                    st.error("Invalid OTP.")

# --- Cookie Manager ---
cookies = EncryptedCookieManager(
    prefix="contract_intel_",  
    password="super_secret_key_here"  # Change to secure random string
)
if not cookies.ready():
    st.stop()

# Default to upload page if no cookie exists
current_page = cookies.get("current_page", "Upload")

# --- Session State Restore ---
if "user" not in st.session_state:
    if cookies.get("user") and cookies.get("username"):
        st.session_state["user"] = cookies.get("user")
        st.session_state["username"] = cookies.get("username")
        st.session_state["authenticated"] = True
    else:
        st.session_state["authenticated"] = False

# --- LOGIN PAGE ---
def login_page():
    st.subheader("Login")
    with st.form(key="login_form", clear_on_submit=True):
        username = st.text_input("Username", key="login_username")
        password = st.text_input("Password", type="password", key="login_pass")
        col1, col2 = st.columns([2, 1])
        login_clicked = col1.form_submit_button("Login")
        forgot_clicked = col2.form_submit_button("Forgot Password")
        if login_clicked:
            user = get_user_by_username(username)
            if user is None:
                st.error("User not found.")
            elif user["password_hash"] != hash_password(password):
                st.error("Incorrect password.")
            elif not user["is_verified"]:
                st.error("User not verified. Please register and verify OTP.")
            else:
                st.session_state["authenticated"] = True
                st.session_state["user"] = user["email"]
                st.session_state["username"] = user["username"]
                cookies["user"] = user["email"]
                cookies["username"] = user["username"]
                cookies.save()
                st.success("Login successful!")
                time.sleep(0.5)
                st.rerun()
        if forgot_clicked:
            st.session_state["show_forgot_password"] = True
            st.session_state["authenticated"] = False
            st.rerun()

# --- LOGOUT ---
def logout():
    for key in ["authenticated", "user", "username", "show_forgot_password"]:
        if key in st.session_state:
            del st.session_state[key]
    cookies["user"] = ""
    cookies["username"] = ""
    cookies.save()
    st.success("Logged out successfully!")
    time.sleep(0.5)
    st.rerun()

def forgot_password_page():
    st.subheader("Forgot Password")
    if "fp_stage" not in st.session_state:
        st.session_state["fp_stage"] = "input_user_email"
    with st.form(key="forgot_form", clear_on_submit=True):
        if st.session_state["fp_stage"] == "input_user_email":
            username = st.text_input("Username", key="fp_username")
            email = st.text_input("Registered Gmail", key="fp_email")
            if st.form_submit_button("Send OTP for Reset"):
                user = get_user_by_username(username)
                if user is None:
                    st.error("User not found.")
                elif user["email"] != email:
                    st.error("Email does not match the username.")
                else:
                    otp = generate_otp()
                    update_user_otp(email, otp)
                    if send_otp_email(email, otp):
                        st.session_state["fp_pending_email"] = email
                        st.session_state["fp_stage"] = "otp_and_password"
                        st.success("OTP sent to your Gmail.")
                    else:
                        st.error("Failed to send OTP.")
        if st.session_state.get("fp_stage") == "otp_and_password":
            otp_input = st.text_input("Enter OTP for Password Reset", key="fp_otp")
            new_pass = st.text_input("New Password", type="password", key="fp_newpass")
            confirm_pass = st.text_input("Confirm New Password", type="password", key="fp_confirmpass")
            if st.form_submit_button("Reset Password"):
                user = get_user(st.session_state["fp_pending_email"])
                if user is not None and str(user["otp"]) == otp_input:
                    if new_pass != confirm_pass:
                        st.error("Passwords do not match.")
                    elif len(new_pass) < 6:
                        st.error("Password must be at least 6 characters.")
                    else:
                        update_user_password(user["email"], new_pass)
                        st.success("Password reset successful! You can now login.")
                        del st.session_state["fp_pending_email"]
                        st.session_state["fp_stage"] = "input_user_email"
                else:
                    st.error("Invalid OTP.")

# --- User Data Functions ---
USERS_CSV = "users.csv"

def load_users():
    cols = ["username", "email", "password_hash", "otp", "is_verified"]
    if not os.path.exists(USERS_CSV):
        return pd.DataFrame(columns=cols)
    df = pd.read_csv(USERS_CSV)
    for col in cols:
        if col not in df.columns:
            df[col] = "" if col != "is_verified" else False
    df = df[cols]
    return df

def save_users(df):
    df.to_csv(USERS_CSV, index=False)

def hash_password(password):
    return hashlib.sha256(password.encode()).hexdigest()

def generate_otp():
    return str(random.randint(100000, 999999))

def send_otp_email(receiver_email, otp):
    sender_email = "learnsaxon@gmail.com"  # <-- CHANGE THIS
    sender_password = "ihiumsydgpaceykb"
    subject = "Your OTP Code"
    body = f"Your OTP code is: {otp}"
    msg = MIMEMultipart()
    msg["From"] = sender_email
    msg["To"] = receiver_email
    msg["Subject"] = subject
    msg.attach(MIMEText(body, "plain"))
    try:
        server = smtplib.SMTP("smtp.gmail.com", 587)
        server.starttls()
        server.login(sender_email, sender_password)
        server.sendmail(sender_email, receiver_email, msg.as_string())
        server.quit()
        return True
    except Exception as e:
        print("Email send error:", e)
        return False

def user_exists(email):
    df = load_users()
    return email in df["email"].values

def username_exists(username):
    df = load_users()
    return username in df["username"].values

def get_user(email):
    df = load_users()
    user = df[df["email"] == email]
    return user.iloc[0] if not user.empty else None

def get_user_by_username(username):
    df = load_users()
    user = df[df["username"] == username]
    return user.iloc[0] if not user.empty else None

def update_user_otp(email, otp):
    df = load_users()
    df.loc[df["email"] == email, "otp"] = otp
    save_users(df)

def update_user_password(email, new_password):
    df = load_users()
    df.loc[df["email"] == email, "password_hash"] = hash_password(new_password)
    save_users(df)

def verify_user(email):
    df = load_users()
    df.loc[df["email"] == email, "is_verified"] = True
    save_users(df)

# --- Page Setup ---
lottie_contract = load_lottieurl("https://assets10.lottiefiles.com/packages/lf20_t9gkkhz4.json")

# --- Auth Navigation ---
if not st.session_state.get("authenticated", False):
    st.markdown('<div class="auth-card">', unsafe_allow_html=True)
    if st.session_state.get("show_forgot_password", False):
        forgot_password_page()
        if st.button("Back to Login"):
            st.session_state["show_forgot_password"] = False
            st.rerun()
    else:
        auth_tab = st.sidebar.radio("Authentication", ["Login", "Register", "Forgot Password"])
        if auth_tab == "Login":
            login_page()
        elif auth_tab == "Register":
            register_page()
        elif auth_tab == "Forgot Password":
            st.session_state["show_forgot_password"] = True
            st.rerun()
    st.markdown('</div>', unsafe_allow_html=True)
    st.stop()
else:
    st.sidebar.markdown(f"**Logged in as:** {st.session_state.get('username', '')}")
    if st.sidebar.button("Logout"):
        logout()

# --- Navigation Menu ---
if "current_page" not in st.session_state:
    st.session_state["current_page"] = cookies.get("current_page", "Upload")

with st.sidebar:
    selected = option_menu(
        "Navigation",
        ["Upload", "Analysis", "Insights", "Download"],
        icons=['cloud-upload', 'bar-chart', 'lightbulb', 'download'],
        menu_icon="cast",
        default_index=["Upload", "Analysis", "Insights", "Download"].index(st.session_state["current_page"]),
        key="main_navigation"
    )

if selected != st.session_state["current_page"]:
    st.session_state["current_page"] = selected
    cookies["current_page"] = selected
    cookies.save()
    st.rerun()
st.markdown('<h1>üóê ‚ÑÇùï†ùïüùï•ùï£ùïíùïîùï• ùïÄùïüùï•ùïñùïùùïùùïöùïòùïñùïüùïîùïñ ùïäùï™ùï§ùï•ùïñùïû ùïóùï†ùï£ ùïÉùïñùïòùïíùïù ùïãùïñùïíùïûùï§</h1>', unsafe_allow_html=True) #####################################

# --- Load Legal-BERT Model ---
@st.cache_resource
def load_bert_model():
    model_path = "./legalbert_model"
    if not os.path.exists(model_path):
        st.error(f"Model directory '{model_path}' not found. Please ensure the model files are present.")
        return None, None
    try:
        bert_model = BertForSequenceClassification.from_pretrained(model_path)
        bert_tokenizer = BertTokenizer.from_pretrained(model_path)
        bert_model.eval()
        return bert_model, bert_tokenizer
    except Exception as e:
        st.error(f"Failed to load Legal-BERT model: {e}")
        return None, None

bert_model, bert_tokenizer = load_bert_model()
if not bert_model or not bert_tokenizer:
    st.stop()

# --- Load Legal Summarizer ---
@st.cache_resource
def load_pegasus_model():
    pegasus_model_name = "google/pegasus-xsum"
    try:
        pegasus_tokenizer = AutoTokenizer.from_pretrained(pegasus_model_name)
        pegasus_model = AutoModelForSeq2SeqLM.from_pretrained(pegasus_model_name)
        pegasus_model.eval()
        return pegasus_model, pegasus_tokenizer
    except Exception as e:
        st.error(f"Failed to load PEGASUS model: {e}")
        return None, None

pegasus_model, pegasus_tokenizer = load_pegasus_model()
if not pegasus_model or not pegasus_tokenizer:
    st.stop()

@st.cache_data
def load_label_mapping():
    if not os.path.exists("label_mapping.csv"):
        st.error("`label_mapping.csv` not found. Please provide the mapping file.")
        return None
    try:
        label_map_df = pd.read_csv("label_mapping.csv")
        label_map_df["clause_type"] = label_map_df["clause_type"].str.strip().str.replace('"', '')
        id2label = dict(label_map_df.values)
        return id2label
    except Exception as e:
        st.error(f"Error loading label mapping: {e}")
        return None

id2label = load_label_mapping()
if not id2label:
    st.stop()

# --- Clause Insight Mapping ---
clause_insights = {
    "Confidentiality": "Ensures sensitive information is protected.",
    "Termination for Convenience": "Allows parties to end the contract without cause.",
    "Dispute Resolution": "Specifies how disputes will be resolved.",
    "Cap on Liability": "Limits the amount a party has to pay if things go wrong.",
    "IP Ownership": "Clarifies who owns the intellectual property.",
    "Anti-Assignment": "Restricts transfer of contractual rights.",
    "Governing Law": "Specifies the legal jurisdiction governing the contract.",
    "Post-Termination": "Explains obligations even after the contract ends.",
    "General legal clause": "Standard legal clause without specific risk."
}

# --- Utils ---
@st.cache_data
def extract_deadlines_and_risks(text):
    text_lower = text.lower()
    deadline_patterns = [
        r"(?:within|at least|not less than|no later than)?\s*\(?\d{1,3}\)?\s*(?:days?|weeks?|months?|years?)'?\s*(?:prior to|before|after|preceding|from)?\s*(?:the\s+)?(?:effective\s+date|termination|claim|execution|delivery|notice|event)?",
        r"(?:on|by|before)?\s*(?:jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sep(?:t(?:ember)?)?|oct(?:ober)?|nov(?:ember)?|dec(?:ember)?)\s+\d{1,2},?\s+\d{4}",
        r"(?:\d{1,2}/\d{1,2}/\d{2,4})"
    ]
    deadlines = []
    for pattern in deadline_patterns:
        found = re.findall(pattern, text, flags=re.IGNORECASE)
        for d in found:
            cleaned = d.strip().replace('\n', ' ')
            if len(cleaned) > 5 and not cleaned.lower().startswith(('or', 'and')):
                deadlines.append(cleaned)
    deadline_result = ", ".join(sorted(set(deadlines))) if deadlines else "-"
    risk_keywords = {
        "termination": "termination", "penalty": "penalty", "liability": "liability",
        "breach": "breach", "damages": "damages", "dispute": "dispute",
        "loss": "loss", "indemnity": "indemnity", "transfer": "transfer risk",
        "ownership": "IP risk"
    }
    risks = {label for word, label in risk_keywords.items() if word in text_lower}
    risk_result = ", ".join(sorted(risks)) if risks else "-"
    return deadline_result, risk_result

def generate_summaries_batch(clauses, batch_size=8):
    summaries = []
    for i in range(0, len(clauses), batch_size):
        batch_clauses = clauses[i:i+batch_size]
        filtered_batch = [clause if len(clause) > 100 else "[SHORT]" for clause in batch_clauses]
        inputs = pegasus_tokenizer(filtered_batch, truncation=True, padding="longest", return_tensors="pt")
        with torch.no_grad():
            summary_ids = pegasus_model.generate(**inputs, max_length=60, min_length=20, num_beams=4, early_stopping=True)
        decoded = pegasus_tokenizer.batch_decode(summary_ids, skip_special_tokens=True)
        for original, summary in zip(batch_clauses, decoded):
            summaries.append(original.strip() if len(original) <= 100 else summary.strip())
    return summaries

def generate_insight(clause_text, label=None):
    text = clause_text.lower()
    if label:
        if label == "Confidentiality": return "Ensures sensitive or proprietary information is not disclosed."
        elif label == "Termination for Convenience": return "Allows either party to terminate the agreement without specific cause."
        elif label == "Dispute Resolution": return "Describes how legal conflicts will be resolved, such as arbitration or court."
        elif label == "Cap on Liability": return "Limits the amount one party must pay for damages or losses."
        elif label == "IP Ownership": return "Specifies who holds the rights to intellectual property developed or used."
        elif label == "Anti-Assignment": return "Prevents parties from transferring their rights or obligations to others."
        elif label == "Governing Law": return "Declares which country's/state's laws apply to the contract."
        elif label == "Post-Termination": return "Describes obligations that continue even after the contract ends."
        elif label == "General legal clause": return "Standard clause included for legal completeness."
    if "terminate" in text: return "Clause allows termination of agreement."
    elif "jurisdiction" in text or "governed by" in text: return "Specifies the legal jurisdiction."
    elif "liability" in text: return "Limits financial liability."
    elif "confidential" in text: return "Protects sensitive information."
    elif "intellectual property" in text or "ownership" in text: return "Defines intellectual property rights."
    else: return "General legal clause."

def predict_clauses_batch(clauses):
    tokens = bert_tokenizer(clauses, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = bert_model(**tokens)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        pred_ids = torch.argmax(probs, dim=1)
        confidences = probs.max(dim=1).values
    labels = [id2label[i.item()] for i in pred_ids]
    return labels, confidences.tolist()

def split_into_clauses(text):
    text = re.sub(r'\n+', '\n', text)
    text = re.sub(r'\xa0', ' ', text)
    pattern = r'(?:^|\n)(?=[0-9]+\.\s|[\u2022\-]\s|[A-Z][a-z])'
    clauses = re.split(pattern, text)
    return [c.strip() for c in clauses if len(c.strip()) > 40]

def extract_text(file):
    if file.type == "application/pdf": return extract_text_from_pdf(file)
    elif file.type == "text/plain": return file.read().decode("utf-8")
    elif file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
        doc = docx.Document(file)
        return "\n".join([p.text for p in doc.paragraphs])
    elif file.type == "text/csv":
        df = pd.read_csv(file)
        return "\n".join(df.iloc[:,0].astype(str))
    elif file.type == "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet":
        df = pd.read_excel(file)
        return "\n".join(df.iloc[:,0].astype(str))
    return ""

def extract_text_from_pdf(uploaded_pdf):
    doc = fitz.open(stream=uploaded_pdf.read(), filetype="pdf")
    return "\n".join(page.get_text() for page in doc)

def generate_file_download(dataframe, file_format):
    df_str = dataframe.astype(str)
    if file_format == "PDF":
        pdf = FPDF()
        pdf.set_auto_page_break(auto=True, margin=15)
        pdf.add_page()
        pdf.set_font("Arial", size=10)
        for _, row in df_str.iterrows():
            text = f"Clause Type: {row['Clause Type']}\nConfidence: {row['Confidence (%)']}%\nDeadline: {row['Deadline']}\nRisks: {row['Risks']}\nInsight: {row['Insight']}\nSummary: {row['Summary']}\n\n{row['Clause']}\n"
            pdf.multi_cell(0, 10, text.encode('utf-8').decode('latin-1'), border=1)
            pdf.ln()
        pdf_output = pdf.output(dest='S').encode('utf-8', 'ignore')
        return pdf_output, "application/pdf", "predicted_clauses.pdf"
    elif file_format == "TXT":
        text_data = df_str.to_string(index=False)
        return text_data.encode('utf-8'), "text/plain", "predicted_clauses.txt"
    elif file_format == "CSV":
        return df_str.to_csv(index=False).encode('utf-8'), "text/csv", "predicted_clauses.csv"
    elif file_format == "DOCX":
        doc = docx.Document()
        for _, row in df_str.iterrows():
            doc.add_paragraph(f"Clause Type: {row['Clause Type']}")
            doc.add_paragraph(f"Confidence: {row['Confidence (%)']}%")
            doc.add_paragraph(f"Deadline: {row['Deadline']}")
            doc.add_paragraph(f"Risks: {row['Risks']}")
            doc.add_paragraph(f"Insight: {row['Insight']}")
            doc.add_paragraph(f"Summary: {row['Summary']}")
            doc.add_paragraph(row['Clause'])
            doc.add_paragraph("---------------------------")
        buffer = BytesIO()
        doc.save(buffer)
        return buffer.getvalue(), "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "predicted_clauses.docx"
    elif file_format == "XLSX":
        buffer = BytesIO()
        df_str.to_excel(buffer, index=False)
        return buffer.getvalue(), "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", "predicted_clauses.xlsx"

if selected == "Upload":
    st.markdown('<div class="card">', unsafe_allow_html=True)
    st_lottie(lottie_contract, height=250, key="contract")
    uploaded_file = st.file_uploader("üìéUpload Contract (PDF, DOCX, XLSX, TXT, CSV)", type=["pdf", "txt", "docx", "csv", "xlsx"])
    if uploaded_file:
        raw_text = extract_text(uploaded_file)
        st.text_area("Extracted Text", raw_text[:3000], height=200)
        clauses = split_into_clauses(raw_text)
        st.success(f"‚úì Found {len(clauses)} potential clauses.")
        for i, c in enumerate(clauses[:5]):
            st.write(f"Clause {i+1}: {c[:100]}...")
        st.session_state.clauses = clauses
    st.markdown('</div>', unsafe_allow_html=True)

elif selected == "Analysis":
    st.markdown('<div class="card">', unsafe_allow_html=True)
    st.subheader("‚åï Run Clause Analysis") ############################################################################################3
    if "clauses" in st.session_state and st.session_state.clauses:
        if st.session_state.get("clause_data") is not None:
            st.success("‚úì Analysis has already been run for this document.")
            st.info("You can view the results in the 'Insights' tab or clear the analysis from the sidebar to re-run.")
        else:
            if st.button("‚åï Analyze Clauses"): ########################################################################################
                with st.spinner("Analyzing clauses using Legal-BERT and PEGASUS-XSUM..."):
                    labels, confidences = predict_clauses_batch(st.session_state.clauses)
                    summaries = generate_summaries_batch(st.session_state.clauses)
                    data = []
                    for clause, label, conf, summary in zip(st.session_state.clauses, labels, confidences, summaries):
                        deadline, risks = extract_deadlines_and_risks(clause)
                        insight = clause_insights.get(label, "General legal clause.")
                        data.append({
                            "Clause": clause,
                            "Clause Type": label,
                            "Confidence (%)": round(conf * 100, 2),
                            "Deadline": deadline,
                            "Risks": risks,
                            "Insight": insight,
                            "Summary": summary
                        })
                    result_df = pd.DataFrame(data)
                    st.session_state.clause_data = result_df
                    persist_analysis_to_disk(result_df)
                    st.success("‚úì Analysis completed.")
                    st.rerun()
    else:
        st.warning("‚ö†Ô∏é No document uploaded. Go to 'Upload' to begin.")
    st.markdown('</div>', unsafe_allow_html=True)

elif selected == "Insights":
    st.markdown('<div class="card">', unsafe_allow_html=True)
    if "clause_data" not in st.session_state:
        restored = restore_analysis_from_cookie()
        if restored is not None:
            st.session_state.clause_data = restored
    if "clause_data" in st.session_state:
        result_df = st.session_state.clause_data
        st.markdown("### üìä Overall Insights")
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Total Clauses", len(result_df))
        with col2:
            st.metric("Unique Clause Types", len(result_df["Clause Type"].unique()))
        with col3:
            st.metric("High-Risk Clauses", (result_df["Risks"] != "-").sum())
        with col4:
            st.metric("Clauses with Deadlines", (result_df["Deadline"] != "-").sum())

        st.markdown("---")
        st.markdown("### üìä Full Clause Insights")
        st.dataframe(result_df, use_container_width=True)

        st.markdown("### ‚ö†Ô∏é High Risk Clauses") #################################################################################
        st.dataframe(result_df[result_df["Risks"] != "-"][["Clause", "Risks"]], use_container_width=True)

        st.markdown("### ‚è± Clauses with Deadlines")
        st.dataframe(result_df[result_df["Deadline"] != "-"][["Clause", "Deadline"]], use_container_width=True)

        st.markdown("### üìù Clause Summaries")
        st.dataframe(result_df[["Clause", "Summary"]], use_container_width=True)

        if result_df["Summary"].eq(result_df["Clause"]).all():
            st.warning("‚ö†Ô∏é All summaries are identical to clauses. Try uploading longer clauses or more complex contracts.")
    else:
        st.warning("‚ö†Ô∏é No analyzed data found. Go to 'Analysis' tab.") ######################################################
    st.markdown('</div>', unsafe_allow_html=True)

elif selected == "Download":
    st.markdown('<div class="card">', unsafe_allow_html=True)
    st.subheader("üì• Download Report")
    if "clause_data" in st.session_state and st.session_state.clause_data is not None and not st.session_state.clause_data.empty:
        result_df = st.session_state.clause_data
        file_format = st.selectbox("Select Format", ["PDF", "DOCX", "XLSX", "TXT", "CSV"])
        file_data, mime, filename = generate_file_download(result_df, file_format)
        st.download_button(
            f"‚¨áÔ∏è Download Clause Report ({file_format})",
            file_data,
            filename,
            mime
        )
    else:
        st.warning("‚ö†Ô∏é Nothing to download. Run analysis first.")
    st.markdown('</div>', unsafe_allow_html=True)

# --- Sidebar: Clause Repository ---
st.sidebar.title("üìö Clause Repository")

# üîπ Add Clear Saved Analysis button at the top
if st.sidebar.button("üóëÔ∏è Clear Saved Analysis"):
    clear_persisted_analysis()
    st.session_state.pop("clause_data", None)
    st.success("Analysis data cleared.")

if "clause_data" in st.session_state:
    clause_df = st.session_state.clause_data
    clause_types = sorted(clause_df["Clause Type"].unique())

    # --- Filters ---
    selected_type = st.sidebar.selectbox("‚åï Filter by Clause Type", ["All"] + clause_types)
    search_query = st.sidebar.text_input("‚åï Search Clause or Summary")

    # --- Apply Filters ---
    filtered_df = clause_df.copy()
    if selected_type != "All":
        filtered_df = filtered_df[filtered_df["Clause Type"] == selected_type]
    if search_query:
        filtered_df = filtered_df[
            filtered_df["Clause"].str.contains(search_query, case=False, na=False) |
            filtered_df["Summary"].str.contains(search_query, case=False, na=False)
        ]

    st.sidebar.markdown(f"**üóê Total Filtered Clauses: {len(filtered_df)}**")

    # --- Display Clauses with Full Info ---
    for i, row in filtered_df.iterrows():
        with st.sidebar.expander(f"üóê Clause {i+1}: {row['Clause Type']}"):
            st.markdown(f"**Clause:** {row['Clause']}")
            st.markdown(f"**Summary:** {row['Summary']}")
            st.markdown(f"**Insight:** {row['Insight']}")
            st.markdown(f"**Risks:** {row['Risks'] if row['Risks'] != '-' else 'No specific risk detected'}**")

            # ‚úÖ More meaningful deadline display
            if row['Deadline'] and row['Deadline'] != "-":
                st.markdown(f"**Deadline:** {row['Deadline']} üìÖ")
            else:
                st.markdown("**Deadline:** ‚ùå Not specified")

            st.markdown(f"**Confidence:** {row['Confidence (%)']}%")
else:
    st.sidebar.info("üóê Upload and analyze a document to view repository.")


